# reading a raw data file


import pandas as pd
car_df  = pd.read_csv(""C:\\Users\\Swadhin\\Desktop\\Cars_Retail_Price.csv"")


# Data Exploration:

#STATS OF DATA

# column names
car_df.dtypes 

# data structure:
car_df.info()



# to summarize data frame
analysis=car_df.describe()
pd.DataFrame(analysis)

# exploring categorical variables:


for table in ['Cylinder', 'Doors', 'Cruise', 'Sound','Leather']:
    car_df[table] = car_df[table].astype('category')
    
    
   
cat_vars = car_df.select_dtypes(include='category').columns.tolist()
cat_vars=list(cat_vars)


for i in cat_vars:
    x=car_df[i].value_counts() 
    print(x)
    

# checking if missing values exist?
    

car_df.isnull().sum()

# check for outliers for extreme values



# creating dummy variables(one hot encoding):

# create dummies for categorical variables:

car_df['Make'].value_counts()
dummies = pd.get_dummies(car_df['Make']).rename(columns=lambda x: 'Make_' + str(x))
# bring the dummies back into the original dataset
car_df = pd.concat([car_df, dummies], axis=1)

car_df['Model'].value_counts()
dummies2 = pd.get_dummies(car_df['Model']).rename(columns=lambda x: 'Model_' + str(x))
# bring the dummies back into the original dataset
car_df = pd.concat([car_df, dummies2], axis=1)

car_df['Trim'].value_counts()
dummies3 = pd.get_dummies(car_df['Trim']).rename(columns=lambda x: 'Trim_' + str(x))
# bring the dummies back into the original dataset
car_df = pd.concat([car_df, dummies3], axis=1)


car_df['Type'].value_counts()
dummies4 = pd.get_dummies(car_df['Type']).rename(columns=lambda x: 'Type_' + str(x))
# bring the dummies back into the original dataset
car_df = pd.concat([car_df, dummies4], axis=1)


    
    
# drop columns that are not required- categorical(4) + dummies(dummy variable trap ones+ lower frequency ones)

 
 #checking frequencies and dropping variables:

 
car_df['Make'].value_counts()  # drop Make_Saturn
car_df['Model'].value_counts()  # keep Model_Malibu, Model_Cavalier, Model_AVEO, Model_Cobalt, Model_Ion
car_df['Trim'].value_counts()   # keep Sedan4D, Coupe 2D , LS Sedan 4D, LS Coupe 2D, LT Sedan 4D 
car_df['Type'].value_counts()   #drop convertible
 
 
 #keep shorlisted vars:
   
new_df=car_df[['Price', 'Mileage','Cylinder','Liter','Doors','Cruise','Sound','Leather','Make_Buick','Make_Cadillac','Make_Chevrolet','Make_Pontiac','Make_SAAB','Model_Malibu','Model_Cavalier','Model_AVEO', 'Model_Cobalt',  'Model_Ion','Trim_Sedan 4D','Trim_Coupe 2D','Trim_LS Sedan 4D','Trim_LS Coupe 2D','Trim_LT Sedan 4D', 'Type_Coupe','Type_Hatchback','Type_Sedan','Type_Wagon']]
 
 # removing variables that are near zero/ low standard dev/variance
 
import numpy as np
np.var(new_df, axis=0)

    
### dropping correlated var ####
 
cor=new_df.corr()
 
 cor.to_csv('cor_linear_reg1.csv')   
 import os
 os.getcwd()
 
 new_df_model=new_df.drop(['Trim_Coupe 2D','Type_Coupe'], axis=1)
 
 
 #converting all variables to numeric:
 
model_df = new_df_model.apply(pd.to_numeric)

   

#model iterations:
x = model_df.iloc[:,1:25]
y = model_df.iloc[:,0]


#Splitting the data into training and test sets
from sklearn.cross_validation import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state =64) 


from sklearn.linear_model import LinearRegression
lm = LinearRegression()
lm = lm.fit(x_train,y_train)


#checking coefficients:

lm.coef_

import numpy as np
coefficients = pd.concat([pd.DataFrame(x_train.columns),pd.DataFrame(np.transpose(lm.coef_))], axis = 1)

lm.intercept_


#To predict the values of y on the test set we use lm.predict( )

y_pred = lm.predict(x_test)

#Residuals/Errors are the difference between observed and predicted values.
#y_error = y_test - y_pred

y_pred=pd.DataFrame(y_pred)

y_test.to_csv('y_test.csv') 
y_pred.to_csv('y_pred.csv') 
y_error.to_csv('y_error.csv') 
import os
os.getcwd()


#R square can be obbtained using sklearn.metrics ( ):
from sklearn.metrics import r2_score
r2_score(y_test,y_pred)

############################################################################# end ##############################
import pandas as pd
import numpy as np
from sklearn import datasets, linear_model
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
from scipy import stats

X2 = sm.add_constant(x_train)
est = sm.OLS(X2, y_train)
est2=est.fit()
est2.summary()